{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "import time #helper libraries\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transform_data():\n",
    "    \"\"\"\n",
    "    Load energy and weather data sets for energy consumption over a 4 year period for a single household in Austin, Texas. \n",
    "    Perform transformations in order to get data into a format that we can more easily use for LSTM modeling.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Links to source of data:\n",
    "    https://www.kaggle.com/srinuti/residential-power-usage-3years-data-timeseries?select=weather_2016_2020_daily.csv\n",
    "    \n",
    "    Parameters \n",
    "    ------\n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas dataframe\n",
    "        Contains both the energy and weather feature sets \n",
    "    \"\"\"\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load energy data\n",
    "energy_data = \"./Energy_Data/energy_usage_2016_to_2020.csv\"\n",
    "df_energy = pd.read_csv(energy_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load weather data\n",
    "weather_data = \"./Energy_Data/weather_2016_2020_daily.csv\"\n",
    "df_weather = pd.read_csv(weather_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge these data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Explore data with some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature correlations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which features you want to input into the model \n",
    "input_cols = []\n",
    "df = df[input_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(df):\n",
    "    \"\"\"\n",
    "    Scale time series features, save the scaler function for each feature in a look up dict for inverse scaling post model training. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: pandas datafarme \n",
    "        Contains scaled features\n",
    "        \n",
    "    scaler_dict: dict \n",
    "        look up for feature scaler objects \n",
    "        key: column name\n",
    "        value: column scaler \n",
    "    \"\"\"\n",
    "    \n",
    "    # use to save the column scaler functions\n",
    "    scaler_dict = {} \n",
    "    \n",
    "    # use to save the scaled column data\n",
    "    scaled_data = {} \n",
    "    \n",
    "    for col in df.columns:\n",
    "        \n",
    "        # instantiate the scaler class \n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        \n",
    "        # reshape to avoid shape errors\n",
    "        feat = df[col].values.reshape(-1, 1)\n",
    "        \n",
    "        # scale data\n",
    "        col_scaled = scaler.fit_transform(feat)\n",
    "        \n",
    "        # save scaled column data to column key\n",
    "        scaled_data[col] = col_scaled.flatten()\n",
    "        \n",
    "        # save scaler function to column key \n",
    "        scaler_dict[col] = scaler\n",
    "        \n",
    "    # move scaled data from dict to dataframe\n",
    "    df_scaled = pd.DataFrame.from_dict(scaled_data)\n",
    "    \n",
    "    return df_scaled, scaler_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot non-scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Create Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, look_back=1, look_ahead=1):\n",
    "    \"\"\"\n",
    "    Takes a 2D array of sequential data and creates (X) input and (Y) output sequences for timeseries forecasting.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    data: 2D numpy array\n",
    "        contains sequential data with n rows and m columns\n",
    "        \n",
    "        If you provide more than a single input feature, make sure that the feature that is also used for the output\n",
    "        is positioned in the 0 (zero) index column of the the array. Code below assumes this to be the case! \n",
    "        \n",
    "        Example\n",
    "        -------\n",
    "        For the example data below, kWh column will be used as the output feature (i.e. what the model will be predicting).\n",
    "        \n",
    "                    kWh     Temp_avg\n",
    "            0\t0.335705\t0.785256\n",
    "            1\t0.526399\t0.951923\n",
    "            2\t0.697993\t0.961538\n",
    "            3\t0.467702\t0.891026\n",
    "            4\t0.399143\t0.842949      \n",
    "        \n",
    "    look_back: int or float\n",
    "        length of input sequence \n",
    "        \n",
    "        I.E if look_behind = 1, then the input is a single value with index [t] (the present timestep)\n",
    "            if look_behind = 2, then the input is a sequence of vlaues with indices [t - 1, t]\n",
    "            if look_behind = n, then the input is a sequence of values with indices [t - n, t - (n - 1), ... , t - 1, t]\n",
    "            \n",
    "    look_ahead: int or float\n",
    "        number of steps into the future that we want to forecast\n",
    "        \n",
    "        I.E. if look_ahead = 1, then we want to forecast the value 1 timestep from now,  t + 1\n",
    "             if look_ahead = 2, then we want to forecast the value 2 timesteps from now, t + 2\n",
    "             if look_ahead = b, then we want to forecast the value n timesteps from now, t + n\n",
    "    \"\"\"\n",
    "    # use to store new sequence samples \n",
    "    X_data, Y_data = [], []\n",
    "    \n",
    "    # number of original samples in dataset\n",
    "    n_samples = len(data)\n",
    "    \n",
    "    # avoid an index error in the for loop by capping the max index value \n",
    "    # since we are using look_back & look_ahead number of samples for input and output sequences, respectively\n",
    "    truncate = look_back + look_ahead \n",
    "    \n",
    "    # number of new samples to create\n",
    "    # we must subtract truncate from the number of new samples because the size of the look_back & look_ahead windows \n",
    "    n_new_samples = n_samples - (truncate + 1)\n",
    "    \n",
    "    # if user provided data with more than a single input feature\n",
    "    # then index for the feature in the 0 index and use that as the output feature\n",
    "    if data.shape[1] > 1:\n",
    "        y_data = data[:, 0]\n",
    "        \n",
    "    # if user provided a single feature dataset, leave data set as is \n",
    "    else:\n",
    "        y_data = data\n",
    "        \n",
    "    # LOGIC FOR CREATING X AND Y \n",
    "    # we are going to create n_new_samples number of samples for our model \n",
    "    for i in range(n_new_samples):\n",
    "        \n",
    "        # create input sequence sample \n",
    "        x = data[i : i+look_back]\n",
    "        \n",
    "        # create output sequence sample \n",
    "        y = y_data[i+look_back : i + look_back + look_ahead ]\n",
    "        y = y[-1] # y is actually a sequnce, so we need to take the last value as our output (i.e. we are predicting a single value, not multiple)\n",
    "        \n",
    "        # append new samples to lists\n",
    "        X_data.append(x)\n",
    "        Y_data.append(y)\n",
    "        \n",
    "    return np.array(X_data), np.array(Y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.stack.imgur.com/fXZ6k.png )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a small and simple dummy data set as input for `create_data` in order to see how the input and output data are structured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy array to test how function works\n",
    "# note: the values here are completely arbitrary\n",
    "# but using small, sequential integers helps us understanding the structuring of the sequences \n",
    "n = 11\n",
    "dummy_data = np.arange(1, n)\n",
    "dummy_data =dummy_data.reshape((dummy_data.shape[0], 1))\n",
    "dummy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data, Y_data = create_dataset(dummy_data, \n",
    "                                look_back=1, \n",
    "                                look_ahead=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_split(df, look_back = 1, look_ahead = 1, train_size = 0.60):\n",
    "    \"\"\"\n",
    "    Creates a train test split for sequential data used for time series forecasting. \n",
    "    \"\"\"\n",
    "    \n",
    "    # calculate the number of training samples \n",
    "    n_samples = df.shape[0]\n",
    "    train_size = int(n_samples * train_size)\n",
    "\n",
    "    # samples between the zero and train_size indices are training samples \n",
    "    train = df.iloc[:train_size].values\n",
    "    \n",
    "    # samples between the train_size and the nth index are test samples \n",
    "    test = df.iloc[train_size:].values\n",
    "\n",
    "    # create input and output splits \n",
    "    X_train, Y_train = create_dataset(train, look_back=look_back, look_ahead=look_ahead)\n",
    "    X_test, Y_test = create_dataset(test, look_back=look_back, look_ahead=look_ahead)\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and Y train/test sets \n",
    "look_back = 14\n",
    "look_ahead = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model architecture is arbitrary - you can experiment with different architectures to see how it affects the score (i.e. gridsearch)\n",
    "n_feats = len(input_cols)\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "dropout_prob = 0.5\n",
    "\n",
    "\n",
    "# Create and train model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Visualize model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_loss_metrics(history):\n",
    "    \"\"\"\n",
    "    Use the model history callback to plot the train and test losses vs epochs as well as metrics vs. epochs \n",
    "    \"\"\"\n",
    "    \n",
    "    # plot training and test loss scores \n",
    "    test_loss = history.history[\"val_loss\"]\n",
    "    train_loss = history.history[\"loss\"]\n",
    "    \n",
    "    test_mse = history.history[\"val_mean_squared_error\"]\n",
    "    train_mse = history.history[\"mean_squared_error\"]\n",
    "    \n",
    "    test_mae = history.history[\"val_mean_absolute_error\"]\n",
    "    train_mae = history.history[\"mean_absolute_error\"]\n",
    "    \n",
    "    n_epochs = len(test_loss) + 1\n",
    "    epoches = np.arange(1,  n_epochs)\n",
    "    y_ticks = np.arange(0, 1, 11)\n",
    "\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title(\"Loss vs. Number of Epochs\")\n",
    "    plt.plot(epoches, test_loss, label = \"Test Loss\")\n",
    "    plt.plot(epoches, train_loss, label = \"Train Loss\")\n",
    "    plt.xlim(1,20)\n",
    "    plt.xticks(epoches)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title(\"mean_squared_error vs. Number of Epochs\")\n",
    "    plt.plot(epoches, test_mse, label = \"Test Loss\")\n",
    "    plt.plot(epoches, train_mse, label = \"Train Loss\")\n",
    "    plt.xlim(1,20)\n",
    "    plt.xticks(epoches)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title(\"mean_absolute_error vs. Number of Epochs\")\n",
    "    plt.plot(epoches, test_mae, label = \"Test Loss\")\n",
    "    plt.plot(epoches, train_mae, label = \"Train Loss\")\n",
    "    plt.xlim(1,20)\n",
    "    plt.xticks(epoches)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, X_test):\n",
    "    # make predictions on train and test inputs \n",
    "    y_train_predict = model.predict(X_train)\n",
    "    y_test_predict = model.predict(X_test)\n",
    "    \n",
    "    return y_train_predict, y_test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_test and Y_train can be passed through this function only once\n",
    "# predictions can be passed in here multiple times \n",
    "def inverse_scaling(data, scaler_dict, output_feat_name):\n",
    "    \"\"\"\n",
    "    Now that we have trained our model on scaled data (and made predictions on the scaled test data)\n",
    "    we need to invert the scaling so we have have the data in it's original formate for interpretability. \n",
    "    \"\"\"\n",
    "    return scaler_dict[output_feat_name].inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape Y_train and Y_test so number of rows appers first, i.e. (rows,cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inverse scaling of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Plot Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(Y_train, y_train_predict, Y_test, y_test_predict):\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title(\"Training Set: True vs Predicted kWh\")\n",
    "    plt.grid()\n",
    "    plt.plot(y_train_predict, label = \"Predict\", c=\"r\")\n",
    "    plt.plot(Y_train, label= \"True\", c=\"c\")\n",
    "    plt.xlim((0,300))\n",
    "    plt.legend();\n",
    "    \n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.title(\"Test Set: True vs Predicted kWh\")\n",
    "    plt.grid()\n",
    "    plt.plot(y_test_predict, label = \"Predict\", c=\"r\")\n",
    "    plt.plot(Y_test, label= \"True\", c=\"c\")\n",
    "    plt.xlim((0,300))\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Compare Model against a Baseline \n",
    "\n",
    "One commen baseline to use is to simply shift the kWh values up by `look_ahead` amount and compare that to the true values that occur an a date. In order words, assume that the kWh consumption tomorrow will be the same as the energy consumption to today. So then we can ask the question \"can the model perform better than simply assuming that tomorrow will look like today?\"\n",
    "\n",
    "Of course since `look_ahead` can be any value (not just 1) then the navie guess becomes \"assume that the kWh consumption X number of days from now be the same as today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use root mean squared error as metric for comparison \n",
    "train_score = math.sqrt(mean_squared_error(Y_train, y_train_predict))\n",
    "print('Train Score: %.4f RMSE' % (train_score))\n",
    "\n",
    "test_score = math.sqrt(mean_squared_error(Y_test, y_test_predict))\n",
    "print('Test Score: %.4f RMSE' % (test_score))\n",
    "\n",
    "navie_score = math.sqrt(mean_squared_error(...?, ...?))\n",
    "print('Naive baseline Score: %.4f RMSE' % (test_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
